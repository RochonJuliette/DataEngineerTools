{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction\n",
    "\n",
    "[Scrapy](https://scrapy.org/) est un framework permettant de crawler des\n",
    "sites web et d'en extraire les données de façon structurée.\n",
    "\n",
    "## Installation\n",
    "\n",
    "Nous travaillerons dans un environnement\n",
    "[Anaconda](https://www.anaconda.com/download/), déjà présent sur les\n",
    "machines de l'ESIEE. Sur vos machines personnelles, télécharger la\n",
    "distribution correspondant à la version la plus récente de Python.\n",
    "\n",
    "[Scrapy](https://scrapy.org/) ne fait pas partie de la distribution par\n",
    "défaut de Python et doit être installé manuellement. Ici, le package est\n",
    "déjà installé grâce à Pipenv.\n",
    "\n",
    "Si vous avez besoin d'installer dans un autre cadre.\n",
    "\n",
    "-   Avec **Pipenv** : `pipenv install scrapy`\n",
    "-   Avec **Anaconda** : `conda install -c conda-forge scrapy`\n",
    "\n",
    "Tester la réussite de l'opération dans un interpréteur Python. Avant\n",
    "installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in c:\\users\\julie\\anaconda3\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from scrapy) (1.0.4)\n",
      "Requirement already satisfied: zope.interface>=4.1.3 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from scrapy) (5.2.0)\n",
      "Requirement already satisfied: Twisted>=17.9.0 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from scrapy) (20.3.0)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from scrapy) (2.0.5)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from scrapy) (0.2.0)\n",
      "Requirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in c:\\users\\julie\\anaconda3\\lib\\site-packages (from scrapy) (4.5.0)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from scrapy) (19.1.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from scrapy) (1.5.0)\n",
      "Requirement already satisfied: cryptography>=2.0 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from scrapy) (2.8)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from scrapy) (0.1.16)\n",
      "Requirement already satisfied: service-identity>=16.0.0 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from scrapy) (18.1.0)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from scrapy) (1.1.0)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from scrapy) (1.6.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from scrapy) (1.22.0)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (0.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\julie\\anaconda3\\lib\\site-packages (from zope.interface>=4.1.3->scrapy) (45.2.0.post20200210)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (20.0.1)\n",
      "Requirement already satisfied: incremental>=16.10.1 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (17.5.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (19.3.0)\n",
      "Requirement already satisfied: Automat>=0.3.0 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (20.2.0)\n",
      "Requirement already satisfied: PyHamcrest!=1.10.0,>=1.9.0 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (2.0.2)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from pyOpenSSL>=16.2.0->scrapy) (1.14.0)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.14.0)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\julie\\anaconda3\\lib\\site-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
      "Requirement already satisfied: idna>=2.5 in c:\\users\\julie\\anaconda3\\lib\\site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.8)\n",
      "Requirement already satisfied: pycparser in c:\\users\\julie\\anaconda3\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.19)\n"
     ]
    }
   ],
   "source": [
    "!pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "[Scrapy](https://scrapy.org/) est un framework comportant plusieurs\n",
    "composants.\n",
    "\n",
    "<img src=\"images/architecture.png\" alt=\"image\" class=\"align-center\" />\n",
    "\n",
    "L'ensemble du processus est contrôlé par l'**engine** (les termes anglo\n",
    "saxons ont été retenus pour un meilleur référencement dans la\n",
    "[documentation officielle](https://docs.scrapy.org/en/latest/)).\n",
    "\n",
    "Le framework est articulé avec plusieurs composants qui gèrent chacun un\n",
    "rôle différent. Nous allons les détailler.\n",
    "\n",
    "-   Les **Spiders** : permettent de naviguer sur un site et de\n",
    "    référencer les règles d'extraction de la donnée.\n",
    "-   Les **Pipelines** : font le lien entre la donnée brute et des objets\n",
    "    structurés\n",
    "-   Les **Middlewares** : permettent d'effectuer des transformations sur\n",
    "    les objets ou sur les requêtes exécutées par l'engine.\n",
    "-   Le **Scheduler** : gère l'ordre et le timing des requêtes\n",
    "    effectuées.\n",
    "\n",
    "## Fonctionnement\n",
    "\n",
    "[Scrapy](https://scrapy.org/) est entièrement organisé autour d'un\n",
    "composant central : l'*engine*.\n",
    "\n",
    "Le rôle de l'*engine* est de contrôler le flux de données entre les\n",
    "différents composants du système.\n",
    "\n",
    "1.  En particulier, il est chargé de récupérer les *requests* définies\n",
    "    dans les *spiders*\n",
    "2.  Ces *requests* sont ensuite fournies au *scheduler* qui se charge de\n",
    "    leur ordonnancement\n",
    "3.  Les *requests* sont présentées selon cet ordonnancement à\n",
    "    l'*engine*...\n",
    "4.  ... qui les transmet au *downloader*\n",
    "5.  Le *downloader* effectue la *request* et transmet la *response* (le\n",
    "    contenu de la page web) à l'*engine*...\n",
    "6.  ... puis l'envoie au *spider* pour traitement\n",
    "7.  Le *spider* génére des *items* qui sont transmis à l'*engine*\n",
    "8.  Les *items* sont ensuite poussés dans un pipeline pour nettoyage,\n",
    "    validation et stockage\n",
    "\n",
    "Ce processus est répété jusqu'à épuisement des requêtes.\n",
    "\n",
    "[Scrapy](https://scrapy.org/) est un [framework orienté\n",
    "événements](https://en.wikipedia.org/wiki/Event-driven_architecture)\n",
    "(basé sur [Twisted](https://twistedmatrix.com/)) permettant une\n",
    "programmation asynchrone (non bloquante). C'est particulièrement\n",
    "intéressant dans les opérations de scraping, puisque **le programme\n",
    "n'attend pas le résultat d'une requête pour en lancer une autre**.\n",
    "\n",
    "En effet, lorsque l'on sollicite une ressource (requête réseau, système\n",
    "de fichier, etc.) en mode bloquant, l'exécution du programme est\n",
    "suspendue le temps que la transaction avec la ressource se termine (par\n",
    "exemple le temps qu'une page web soit complètement téléchargée).\n",
    "L'intérêt de faire des appels non bloquants, c'est que l'on peut gérer\n",
    "de multiples téléchargements en parallèle, et que le programme peut\n",
    "continuer à tourner pendant ce temps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un scraping élémentaire\n",
    "\n",
    "Avant de rentrer dans les détails du framework, nous allons mettre en\n",
    "oeuvre un premier script permettant de récupérer l'information présente\n",
    "sur [la page web](http://evene.lefigaro.fr/citations/winston-churchill)\n",
    "recensant les citations de [Sir Winston\n",
    "Churchill](https://en.wikipedia.org/wiki/Winston_Churchill).\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "Examiner le code source de cette page avec l'inspecteur de votre\n",
    "navigateur. Identifier les éléments contenant l'information recherchée,\n",
    "ici la chaîne de caractères contenant la citation proprement dite.\n",
    "\n",
    "## Le code source\n",
    "\n",
    "Le code utilisé est le suivant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load citations_churchill_spider1.py\n",
    "import scrapy\n",
    "\n",
    "class ChurchillQuotesSpider(scrapy.Spider):\n",
    "    name = \"citations de Churchill\"\n",
    "    start_urls = [\"http://evene.lefigaro.fr/citations/winston-churchill\",]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for cit in response.xpath('//div[@class=\"figsco__quote__text\"]'):\n",
    "            text_value = cit.xpath('a/text()').extract_first()\n",
    "            yield { 'text' : text_value }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le fonctionnement\n",
    "\n",
    "Le fonctionnement est le suivant:\n",
    "\n",
    "-   On importe le module [Scrapy](https://scrapy.org/) (3)\n",
    "-   et on définit une sous classe de `scrapy.Spider` (5)\n",
    "-   la variable `start_urls` contient la liste des pages à scraper (7)\n",
    "-   On redéfinit la méthode $parse$ dont la signature est définie dans\n",
    "    la classe mère (9)\n",
    "-   L'objet\n",
    "    [response](https://docs.scrapy.org/en/latest/topics/request-response.html#response-objects)\n",
    "    représente la réponse à la requête HTTP (l'attribut $text$ permet\n",
    "    d'accéder à son contenu). On recherche ensuite tous les containers\n",
    "    `<div>` identifiés dans l'exercice précédent. Ici la page est\n",
    "    particulièrement bien structurée et les citations disposent de leur\n",
    "    propre container, identifié par l'attribut `class` de valeur\n",
    "    `figsco__quote__text`. La sélection se fait par une expression\n",
    "    [XPath](https://en.wikipedia.org/wiki/XPath), un langage de\n",
    "    sélection de noeud dans un document XML (10). En langage naturel, la\n",
    "    requête pourrait se formuler : \"On recherche tous les containers\n",
    "    `<div>` dont la valeur de l'attribut `class` est égal à\n",
    "    `figsco__quote__text`\".\n",
    "-   Pour chaque résultat, on construit un dictionnaire dont la clé est\n",
    "    `text` et la valeur le contenu du lien `<a>`. Ce résultat est fourni\n",
    "    par un générateur ($yield$) (12).\n",
    "\n",
    "On lance le scraping depuis un terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy runspider citations_churchill_spider1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On y trouve des informations sur les paramètres\n",
    "utilisés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2018-01-10 17:21:05 [scrapy.utils.log] INFO: Scrapy 1.4.0 started (bot: scrapybot)\n",
    "2018-01-10 17:21:05 [scrapy.utils.log] INFO: Overridden settings: {'SPIDER_LOADER_WARN_ONLY': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "les\n",
    "[extensions](https://docs.scrapy.org/en/latest/topics/extensions.html)\n",
    "...:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2018-01-10 17:21:05 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.corestats.CoreStats',\n",
    "'scrapy.extensions.telnet.TelnetConsole',\n",
    "'scrapy.extensions.logstats.LogStats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Les composants middleware\n",
    "downloader](https://docs.scrapy.org/en/latest/topics/downloader-middleware.html)\n",
    "... :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2018-01-10 17:21:05 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
    "'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
    "'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
    "'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
    "'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
    "'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
    "'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
    "'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
    "'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
    "'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
    "'scrapy.downloadermiddlewares.stats.DownloaderStats']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idem pour [les composants middleware\n",
    "spider](https://docs.scrapy.org/en/latest/topics/spider-middleware.html)\n",
    "...:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2018-01-10 17:21:05 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    "'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    "'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    "'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    "'scrapy.spidermiddlewares.depth.DepthMiddleware']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aucun\n",
    "[pipeline](https://docs.scrapy.org/en/latest/topics/item-pipeline.html)\n",
    "n'est activé :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2018-01-10 17:21:05 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice**\n",
    "\n",
    "Identifier la position des [composants middleware\n",
    "downloader](https://docs.scrapy.org/en/latest/topics/downloader-middleware.html),\n",
    "des [composants middleware\n",
    "spider](https://docs.scrapy.org/en/latest/topics/spider-middleware.html)\n",
    "et du\n",
    "[pipeline](https://docs.scrapy.org/en/latest/topics/item-pipeline.html)\n",
    "dans $l'architecture <Introduction>$\n",
    "\n",
    "L'exécution du scraping proprement dit débute :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2018-01-10 17:21:05 [scrapy.core.engine] INFO: Spider opened\n",
    "2018-01-10 17:21:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
    "2018-01-10 17:21:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première URL est poussée par le scheduler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2018-01-10 17:21:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://evene.lefigaro.fr/citations/winston-churchill> (referer: None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les résultats\n",
    "\n",
    "Les résultats sont fournis par le générateur défini dans la méthode\n",
    "$parse$ dans un dictionnaire. Ils contiennent le texte des citations\n",
    "dans la valeur de la clé `text` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2018-01-10 17:21:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
    "{'text': '“Le vice inhérent au capitalisme consiste en une répartition inégale des richesses. La vertu inhérente au socialisme consiste en une égale répartition de la misère.”'}\n",
    "2018-01-10 17:21:05 [scrapy.core.scraper] DEBUG: Scraped from <200 http://evene.lefigaro.fr/citations/winston-churchill>\n",
    "{'text': \"Faire le bien, éviter le mal, c'est ça le paradis.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les statistiques\n",
    "\n",
    "Une fois le scraping effectué, quelques statistiques sont affichées sur\n",
    "le terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2018-01-10 17:21:05 [scrapy.core.engine] INFO: Closing spider (finished)\n",
    "2018-01-10 17:21:05 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
    "{'downloader/request_bytes': 242,\n",
    "'downloader/request_count': 1,\n",
    "'downloader/request_method_count/GET': 1,\n",
    "'downloader/response_bytes': 17435,\n",
    "'downloader/response_count': 1,\n",
    "'downloader/response_status_count/200': 1,\n",
    "'finish_reason': 'finished',\n",
    "'finish_time': datetime.datetime(2018, 1, 10, 16, 21, 5, 858347),\n",
    "'item_scraped_count': 16,\n",
    "'log_count/DEBUG': 18,\n",
    "'log_count/INFO': 7,\n",
    "'response_received_count': 1,\n",
    "'scheduler/dequeued': 1,\n",
    "'scheduler/dequeued/memory': 1,\n",
    "'scheduler/enqueued': 1,\n",
    "'scheduler/enqueued/memory': 1,\n",
    "'start_time': datetime.datetime(2018, 1, 10, 16, 21, 5, 645347)}\n",
    "2018-01-10 17:21:05 [scrapy.core.engine] INFO: Spider closed (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe notamment que notre code permet de récupérer la taille de la\n",
    "page web (17435 bytes), le temps d'exécution à partir des valeurs\n",
    "`finish_time` et `start_time`, le nombre d'items scrapés (16), etc...\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "Les citations extraites sont elles toutes de [Sir Winston\n",
    "Churchill](https://en.wikipedia.org/wiki/Winston_Churchill) ? Il sera\n",
    "peut être nécessaire de modifier le sélecteur XPath. Nous verrons ça\n",
    "lorsque il faudra récupérer les données relative à l'auteur.\n",
    "\n",
    "## Modifier les données\n",
    "\n",
    "Il est parfois nécessaire de faire un traitement sur les données\n",
    "scrapées, pour ajouter ou retirer de l'information.\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "Retirer les caractères `“` et `”` qui délimitent la citation. Ces\n",
    "caractères sont identifiés en Unicode comme [LEFT DOUBLE QUOTATION\n",
    "MARK](http://www.fileformat.info/info/unicode/char/201c/index.htm) et\n",
    "[RIGHT DOUBLE QUOTATION\n",
    "MARK](http://www.fileformat.info/info/unicode/char/201d/index.htm).\n",
    "\n",
    "## Plus de données\n",
    "\n",
    "Il est souvent nécessaire de récupérer plusieurs informations relatives\n",
    "à un même item. Dans cet exemple, il est judicieux d'associer à la\n",
    "citation le nom de son auteur, en allant chercher cette information au\n",
    "plus près du texte lui même.\n",
    "\n",
    "**Exercice**\n",
    "\n",
    "Examiner le code source de la page web et identifier la structuration de\n",
    "la donnée associée à l'auteur. En déduire l'expression XPath permettant\n",
    "de la récupérer. S'assurer que seules les citations de [Sir Winston\n",
    "Churchill](https://en.wikipedia.org/wiki/Winston_Churchill) sont\n",
    "extraites. Ajouter une clé `author` au dictionnaire retourné par le\n",
    "$yield$ dont la valeur est précisément la chaîne de caractères contenant\n",
    "l'auteur.\n",
    "\n",
    "Un exemple de dictionnaire retourné:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{   'text': \"“Si deux hommes ont toujours la même opinion, l'un d'eux est de trop.”\", \n",
    "    'author': 'Winston Churchill'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour lancer l'exécution de la spider :\n",
    "\n",
    "> \\$ scrapy runspider spiders/citations\\_churchill\\_spider2.py\n",
    "\n",
    "On peut aussi vouloir stocker les données extraites :\n",
    "\n",
    "> \\$ scrapy runspider spiders/citations\\_churchill\\_spider2.py -o\n",
    "> data/citation.json -t json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Votre premier projet\n",
    "\n",
    "\n",
    "Dans un premier temps vous devez créer un projet Scrapy avec la commande\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy startproject newscrawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette commande va créer un dossier `monprojet` contenant les éléments\n",
    "suivants correspondant au squelette :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newscrawler/\n",
    "    scrapy.cfg            # Options de déploiement\n",
    "\n",
    "    newscrawler/             # Le module Python contenant les informations\n",
    "        __init__.py\n",
    "\n",
    "        items.py          # Fichier contenant les items\n",
    "\n",
    "        middlewares.py    # Fichier contenant les middlewares\n",
    "\n",
    "        pipelines.py      # Fichier contenant les pipelines\n",
    "\n",
    "        settings.py       # Fichier contenant les paramètres du projet\n",
    "\n",
    "        spiders/          # Dossier contenant toutes les spiders\n",
    "            __init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Votre première Spider\n",
    "\n",
    "Une Spider est une classe Scrapy qui permet de mettre en place toute\n",
    "l'architecture complexe vue dans l'introduction. Pour définir une\n",
    "spider, il vous faut hériter de la classe $scrapy.Spider$. La seule\n",
    "chose à faire est de définir la première requête à effectuer et comment\n",
    "suivre les liens. La Spider s'arrêtera lorsqu'elle aura parcouru tous\n",
    "les liens qu'on lui a demandé de suivre.\n",
    "\n",
    "Pour créer une Spider on utilise la syntaxe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy genspider <SPIDER_NAME> <DOMAIN_NAME>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par exemple,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd newscrawler && scrapy genspider lemonde lemonde.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette commande permet de créer une spider appelée `lemonde` pour scraper\n",
    "le domaine `lemonde.fr`. Cela crée le fichier Python\n",
    "`spiders/lemonde.py` suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load newscrawler/newscrawler/spiders/lemonde.py\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class LemondeSpider(scrapy.Spider):\n",
    "    name = 'lemonde'\n",
    "    allowed_domains = ['lemonde.fr']\n",
    "    start_urls = ['http://lemonde.fr/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une bonne pratique pour commencer à développer une Spider est de passer\n",
    "par l'interface Shell proposée par Scrapy. Elle permet de récupérer un\n",
    "objet `Response` et de tester les méthodes de récupération des données."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTENTION : Les commandes scrapy shell doivent être lancées dans un terminal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy shell 'http://lemonde.fr'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les utilisateurs de windows il vous faut mettre des doubles quotes\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy shell \"http://lemonde.fr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy lance un kernel Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2018-12-02 16:05:50 [scrapy.utils.log] INFO: Scrapy 1.3.3 started (bot: newscrawler)\n",
    "2018-12-02 16:05:50 [scrapy.utils.log] INFO: Overridden settings: {'BOT_NAME': 'newscrawler', 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'LOGSTATS_INTERVAL': 0, 'NEWSPIDER_MODULE': 'newscrawler.spiders', 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['newscrawler.spiders']}\n",
    "2018-12-02 16:05:50 [scrapy.middleware] INFO: Enabled extensions:\n",
    "['scrapy.extensions.corestats.CoreStats',\n",
    "'scrapy.extensions.telnet.TelnetConsole']\n",
    "2018-12-02 16:05:50 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
    "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
    "'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
    "'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
    "'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
    "'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
    "'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
    "'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
    "'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
    "'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
    "'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
    "'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
    "2018-12-02 16:05:50 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
    "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
    "'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
    "'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
    "'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
    "'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
    "2018-12-02 16:05:50 [scrapy.middleware] INFO: Enabled item pipelines:\n",
    "[]\n",
    "2018-12-02 16:05:50 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
    "2018-12-02 16:05:50 [scrapy.core.engine] INFO: Spider opened\n",
    "2018-12-02 16:05:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.lemonde.fr/robots.txt> (referer: None)\n",
    "2018-12-02 16:05:50 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.lemonde.fr/> (referer: None)\n",
    "2018-12-02 16:05:54 [traitlets] DEBUG: Using default logger\n",
    "2018-12-02 16:05:54 [traitlets] DEBUG: Using default logger\n",
    "[s] Available Scrapy objects:\n",
    "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
    "[s]   crawler    <scrapy.crawler.Crawler object at 0x10fc38c18>\n",
    "[s]   item       {}\n",
    "[s]   request    <GET https://www.lemonde.fr/>\n",
    "[s]   response   <200 https://www.lemonde.fr/>\n",
    "[s]   settings   <scrapy.settings.Settings object at 0x113bb0898>\n",
    "[s]   spider     <DefaultSpider 'default' at 0x113e60cc0>\n",
    "[s] Useful shortcuts:\n",
    "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
    "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects\n",
    "[s]   shelp()           Shell help (print this help)\n",
    "[s]   view(response)    View response in a browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grâce à cette interface, vous avez accès à plusieurs objets comme la\n",
    "`Response`, la `Request`, la `Spider` par exemple. Vous pouvez aussi\n",
    "exécuter `view(response)` pour afficher ce que Scrapy récupère dans un\n",
    "navigateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [1]: response\n",
    "Out[1]: <200 https://www.lemonde.fr/>\n",
    "\n",
    "In [3]: request\n",
    "Out[3]: <GET https://www.lemonde.fr/>\n",
    "\n",
    "In [4]: type(request)\n",
    "Out[4]: scrapy.http.request.Request\n",
    "\n",
    "In [5]: spider\n",
    "Out[5]: <LemondeSpider 'lemonde' at 0x1080fccc0>\n",
    "\n",
    "In [6]: type(spider)\n",
    "Out[6]: monprojet.spiders.lemonde.LeMondeSpider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on voit que la Spider est une instance de LemondeSpider. Lorsqu'on\n",
    "lance le $scrapy shell$ scrapy va chercher dans les spiders si une\n",
    "correspond au lien passé en paramètre, si oui , il l'utilise sinon une\n",
    "$DefaultSpider$ est instanciée.\n",
    "\n",
    "## Vos premières requêtes\n",
    "\n",
    "On peut commencer à regarder comment extraire les données de la page web\n",
    "en utilisant le langage de requêtes proposé par Scrapy. Il existe deux\n",
    "types de requêtes : les requêtes `css` et `xpath`. Les requêtes `xpath`\n",
    "sont plus complexes mais plus puissantes que les requêtes `css`. Dans le\n",
    "cadre de ce tutorial, nous allons uniquement aborder les requêtes `css`,\n",
    "elles nous suffiront pour extraire les données dont nous avons besoin\n",
    "(en interne, Scrapy transforme les requêtes `css`en requêtes `xpath`.\n",
    "\n",
    "Que ce soit les requêtes `css` ou `xpath`, elles crééent des sélecteurs\n",
    "de différents types. Quelques exemples :\n",
    "\n",
    "Pour récupérer le titre d'une page :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [1]: response.css('title')\n",
    "Out[1]: [<Selector xpath='descendant-or-self::title' data='<title>Le Monde.fr - Actualités et Infos'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère une liste de sélecteurs correspondant à la requête `css`\n",
    "appelée. La requête précédente était unique, d'autres requêtes moins\n",
    "restrictives permettent de récupérer plusieurs résultats. Par exemple\n",
    "pour rechercher l'ensemble des liens présents sur la page, on va\n",
    "rechercher les balises HTML `<a></a>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [5]: response.css(\"a\")[0:10]\n",
    "Out[5]:\n",
    "[<Selector xpath='descendant-or-self::a' data='<a target=\"_blank\" data-target=\"jelec-he'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/\"> <div class=\"logo__lemonde l'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"https://secure.lemonde.fr/sfuse'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"https://abo.lemonde.fr/#xtor=CS'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/\" class=\"Burger__right-arrow j'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/\" class=\"Burger__right-arrow j'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"#\" class=\"js-dropdown Burger__r'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/mouvement-des-gilets-jaunes/\" '>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/carlos-ghosn/\" data-suggestion'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/implant-files/\" data-suggestio'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour récupérer le texte contenu dans les balises, on passe le paramètre\n",
    "`<TAG>::text`. Par exemple :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [6]: response.css(\"title::text\")\n",
    "Out[6]: [<Selector xpath='descendant-or-self::title/text()' data='Le Monde.fr - Actualités et Infos en Fra'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice\n",
    "\n",
    "  Comparer les résultats des deux requêtes `response.css('title')` et\n",
    "`response.css('title::text')`.\n",
    "\n",
    "Maintenant pour extraire les données des selecteurs on utilise l'une des\n",
    "deux méthodes suivantes : - `extract()` permet de récupérer une liste\n",
    "des données extraites de tous les sélecteurs - `extract_first()` permet\n",
    "de récupérer une `String` provenant du premier sélecteur de la liste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [7]: response.css('title::text').extract_first()\n",
    "Out[7]: 'Le Monde.fr - Actualités et Infos en France et dans le monde'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut récupérer un attribut d'une balise avec la syntaxe\n",
    "`<TAG>::attr(<ATTRIBUTE_NAME>)` :\n",
    "\n",
    "Par exemple, les liens sont contenus dans un attribut `href`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [9]: response.css('a::attr(href)')[0:10]\n",
    "Out[9]:\n",
    "[<Selector xpath='descendant-or-self::a/@href' data='https://journal.lemonde.fr'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='/'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='https://secure.lemonde.fr/sfuser/connexi'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='https://abo.lemonde.fr/#xtor=CS1-454[CTA'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='/'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='/'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='#'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='/mouvement-des-gilets-jaunes/'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='/carlos-ghosn/'>,\n",
    "<Selector xpath='descendant-or-self::a/@href' data='/implant-files/'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme vu précédemment, si on veut récupérer la liste des liens de la page on applique la méthode $extract()$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [11]: response.css('a::attr(href)').extract()[0:10]\n",
    "Out[11]:\n",
    "['https://journal.lemonde.fr',\n",
    "'/',\n",
    "'https://secure.lemonde.fr/sfuser/connexion',\n",
    "'https://abo.lemonde.fr/#xtor=CS1-454[CTA_LMFR]-[HEADER]-5-[Home]',\n",
    "'/',\n",
    "'/',\n",
    "'#',\n",
    "'/mouvement-des-gilets-jaunes/',\n",
    "'/carlos-ghosn/',\n",
    "'/implant-files/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les liens dans une page HTML sont souvent codés de manière relative par\n",
    "rapport à la page courante. La méthode de l'objet `Response` peut être\n",
    "utilisée pour recréer l'url complet.\n",
    "\n",
    "Un exemple sur le 4e élément :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [14]: response.urljoin(response.css('a::attr(href)').extract()[8])\n",
    "Out[14]: 'https://www.lemonde.fr/carlos-ghosn/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alors que"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [15]: response.css('a::attr(href)').extract()[8]\n",
    "Out[15]: '/carlos-ghosn/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice : \n",
    "\n",
    "Utiliser une liste compréhension pour transformer les 10\n",
    "premiers liens relatifs récupérés par la méthode `extract()` en liens\n",
    "absolus.    \n",
    "\n",
    "Le résultat doit ressembler à :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Out[23]: \n",
    "['https://journal.lemonde.fr',\n",
    "'https://www.lemonde.fr/',\n",
    "'https://secure.lemonde.fr/sfuser/connexion',\n",
    "'https://abo.lemonde.fr/#xtor=CS1-454[CTA_LMFR]-[HEADER]-5-[Home]',\n",
    "'https://www.lemonde.fr/',\n",
    "'https://www.lemonde.fr/',\n",
    "'https://www.lemonde.fr/',\n",
    "'https://www.lemonde.fr/mouvement-des-gilets-jaunes/',\n",
    "'https://www.lemonde.fr/carlos-ghosn/',\n",
    "'https://www.lemonde.fr/implant-files/']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Des requêtes plus complexes\n",
    "\n",
    "On peut créer des requêtes plus complexes en utilisant à la fois la\n",
    "structuration HTML du document mais également la couche de présentation\n",
    "CSS. On utilise l'inspecteur de `Google Chrome` pour identifier le type\n",
    "et l'identifiant de la balise contenant les informations.\n",
    "\n",
    "Il y a au moins deux choses à savoir en `css` :  \n",
    "\n",
    "-   Les `.` représentent les classes\n",
    "-   Les `#` représentent les id\n",
    "\n",
    "On se propose de récupérer toutes les sous-catégories de news dans la\n",
    "catégorie **Actualités**. On remarque en utilisant l'inspecteur\n",
    "d'élement de Chrome que toutes les catégories sont rangées dans une\n",
    "balise avec l'id $#nav-markup$ ensuite dans les classes $Nav__item$.\n",
    "\n",
    "A partir de cette structure HTML on peut construire la requête suivante\n",
    "pour récupérer la barre de navigation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [19]: response.css(\"#nav-markup\")\n",
    "Out[19]: [<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']\" data='<ul id=\"nav-markup\"> <li class=\"Nav__ite'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite pour récupérer les différentes catégories :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [24]: response.css(\"#nav-markup .Nav__item\")\n",
    "Out[24]:\n",
    "[<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item js-burger-to-show N'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item Nav__item--home Nav'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"/\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>,\n",
    "<Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"/recherc'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut maintenant retourner tous les liens présents dans cette\n",
    "catégorie. On remarque qu'elle apparait à la 4eme position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [34]: response.css(\"#nav-markup .Nav__item\")[3]\n",
    "Out[34]: <Selector xpath=\"descendant-or-self::*[@id = 'nav-markup']/descendant-or-self::*/*[@class and contains(concat(' ', normalize-space(@class), ' '), ' Nav__item ')]\" data='<li class=\"Nav__item\"> <a href=\"#\" class'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant pour récupérer tous les liens on peut chainer les requêtes.\n",
    "On accède alors à toutes les balises $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [35]: response.css(\"#nav-markup .Nav__item\")[3].css(\"a\")\n",
    "Out[35]:\n",
    "[<Selector xpath='descendant-or-self::a' data='<a href=\"#\" class=\"js-dropdown Burger__r'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/mouvement-des-gilets-jaunes/\" '>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/carlos-ghosn/\" data-suggestion'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/implant-files/\" data-suggestio'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/climat/\" data-suggestion>Clima'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/affaire-khashoggi/\" data-sugge'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/emmanuel-macron/\" data-suggest'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/ukraine/\" data-suggestion>Ukra'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/russie/\" data-suggestion>Russi'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/referendum-sur-le-brexit/\" dat'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/harcelement-sexuel/\" data-sugg'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/actualite-en-continu/\" data-su'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/international/\">International<'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/politique/\">Politique</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/societe/\">Société</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/les-decodeurs/\">Les Décodeurs<'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/sport/\">Sport</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/planete/\">Planète</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/sciences/\">Sciences</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/campus/\">M Campus</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/afrique/\">Le Monde Afrique</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/pixels/\">Pixels</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/actualite-medias/\">Médias</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/sante/\">Santé</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/big-browser/\">Big Browser</a>'>,\n",
    "<Selector xpath='descendant-or-self::a' data='<a href=\"/disparitions/\">Disparitions</a'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et pour récupérer les titres :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [37]: response.css(\"#nav-markup .Nav__item\")[3].css(\"a::text\").extract()\n",
    "Out[37]:\n",
    "['Actualités',\n",
    "'Mouvement des \"gilets jaunes\"',\n",
    "'Carlos Ghosn',\n",
    "'Implant Files',\n",
    "'Climat',\n",
    "'Affaire Khashoggi',\n",
    "'Emmanuel Macron',\n",
    "'Ukraine',\n",
    "'Russie',\n",
    "'Brexit',\n",
    "'Harcèlement sexuel',\n",
    "'Toute l’actualité en continu',\n",
    "'International',\n",
    "'Politique',\n",
    "'Société',\n",
    "'Les Décodeurs',\n",
    "'Sport',\n",
    "'Planète',\n",
    "'Sciences',\n",
    "'M Campus',\n",
    "'Le Monde Afrique',\n",
    "'Pixels',\n",
    "'Médias',\n",
    "'Santé',\n",
    "'Big Browser',\n",
    "'Disparitions']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le shell Scrapy permet de définir la structure des requêtes et de\n",
    "s'assurer de la pertinence du résultat retourné. Pour automatiser le\n",
    "processus, il faut intégrer cette syntaxe au code Python des modules de\n",
    "spider définis dans la structure du projet.\n",
    "\n",
    "## Intégration des requêtes\n",
    "\n",
    "Le squelette de la classe `LeMondeSpider` généré lors de la création du\n",
    "projet doit maintenant être enrichi. Par défaut 3 attributs et une\n",
    "méthode `parse()` ont été créés :\n",
    "\n",
    "-   `name` permet d'identifier sans ambiguïté la spider dans le code.\n",
    "-   `allowed_domain` permet de filtrer les requêtes et forcer la spider\n",
    "    à rester sur une liste de domaines.\n",
    "-   `starts_urls` est la liste des urls d'où la spider va partir pour\n",
    "    commencer son scraping.\n",
    "-   `parse()` est une méthode héritée de la classe `scrapy.Spider`. Elle\n",
    "    doit être redéfinie selon les requêtes que l'on doit effectuer et\n",
    "    sera appelée sur l'ensemble des urls contenus dans la liste\n",
    "    `starts_urls`.\n",
    "\n",
    "`parse()` est une fonction `callback` qui sera appelée automatiquement\n",
    "sur chaque objet `Response` retourné par la requête. Cette fonction est\n",
    "appelée de manière asynchrone. Plusieurs requêtes peuvent ainsi être\n",
    "lancées en parallèles sans bloquer le thread principal. L'objet\n",
    "`Response` passé en paramètre est le même que celui mis à disposition\n",
    "lors de l'exécution du Scrapy Shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(self, response):\n",
    "    title = response.css('title::text').extract_first()\n",
    "    all_links = {\n",
    "        name:response.urljoin(url) for name, url in zip(\n",
    "        response.css(\"#nav-markup .Nav__item\")[3].css(\"a::text\").extract(),\n",
    "        response.css(\"#nav-markup .Nav__item\")[3].css(\"a::attr(href)\").extract())\n",
    "    }\n",
    "    yield {\n",
    "        \"title\":title,\n",
    "        \"all_links\":all_links\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction est un générateur (`yield`) et retourne un dictionnaire\n",
    "composé de deux éléments :\n",
    "\n",
    "-   Le titre de la page;\n",
    "-   La liste des liens sortants sous forme de String.\n",
    "\n",
    "Pour le moment cette spider ne parcourt que la page d'accueil, ce qui\n",
    "n'est pas très productif.\n",
    "\n",
    "## Votre premier scraper\n",
    "\n",
    "Récupérer les données sur un ensemble de pages webs nécessite d'explorer\n",
    "en profondeur la structure du site en suivant tout ou partie des liens\n",
    "rencontrés.\n",
    "\n",
    "La spider peut se `balader` sur un site assez efficacement. Il suffit de\n",
    "lui indiquer comment faire. Il faut spécifier à Scrapy de générer une\n",
    "requête vers une nouvelle page en construisant l'objet `Request`\n",
    "correspondant. Ce nouvel objet `Request` est alors inséré dans le\n",
    "scheduler de Scrapy. On peut évidemment générer plusieurs `Request`\n",
    "simultanément, correspondant par exemple, à différents liens sur la page\n",
    "courante. Ils sont insérés séquentiellement dans le scheduler.\n",
    "\n",
    "Pour cela on modifie la méthode `parse()` de façon à ce qu'elle retourne\n",
    "un objet `Request` pour chaque nouveau lien rencontré. On associe\n",
    "également à cet objet une fonction de callback qui déterminera la\n",
    "manière dont cette nouvelle page doit être extraite.\n",
    "\n",
    "Par exemple, pour que la spider continue dans les liens des différentes\n",
    "régions (pour l'instant la fonction de callback ne fait rien) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load newscrawler/newscrawler/spiders/lemonde_v2.py\n",
    "import scrapy\n",
    "from scrapy import Request\n",
    "\n",
    "\n",
    "class LemondeSpider(scrapy.Spider):\n",
    "    name = \"lemondev2\"\n",
    "    allowed_domains = [\"www.lemonde.fr\"]\n",
    "    start_urls = ['https://www.lemonde.fr']\n",
    "\n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').extract_first()\n",
    "        all_links = {\n",
    "            name:response.urljoin(url) for name, url in zip(\n",
    "            response.css(\"#nav-markup .Nav__item\")[3].css(\"a::text\").extract(),\n",
    "            response.css(\"#nav-markup .Nav__item\")[3].css(\"a::attr(href)\").extract())\n",
    "        }\n",
    "        yield {\n",
    "            \"title\":title,\n",
    "            \"all_links\":all_links\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour lancer la spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd newscrawler && scrapy crawl lemondev2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut ensuite *entrer* dans les liens des différentes sous-catégories\n",
    "pour récupérer les articles. Pour cela, nous créons une méthode\n",
    "`parse_category()` prend en argument un objet `Response` qui sera la\n",
    "réponse correspondant aux liens des régions. On peut comme ceci\n",
    "traverser un site en définissant des méthodes différentes en fonction du\n",
    "type de contenu.\n",
    "\n",
    "Si la structure du site est plus profonde, on peut empiler autant de\n",
    "couches que souhaité.\n",
    "\n",
    "Quand on arrive sur une page d'une sous-catégorie, on peut vouloir\n",
    "récupérer tous les éléments de la page. Pour cela, on réutilise le\n",
    "scrapy Shell pour commencer le développement de la nouvelle méthode\n",
    "d'extraction.\n",
    "\n",
    "Par exemple pour la page `https://www.lemonde.fr/international/` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy shell 'https://www.lemonde.fr/international/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fil des articles est stocké dans une balise avec la classe\n",
    "`class=river`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [3]: response.css(\".river\")\n",
    "Out[3]:\n",
    "[<Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' fleuve ')]\" data='<div class=\"fleuve\">\\n   <section>\\n      '>,\n",
    "<Selector xpath=\"descendant-or-self::*[@class and contains(concat(' ', normalize-space(@class), ' '), ' fleuve ')]\" data='<div class=\"fleuve\">\\n</div>'>]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour récupérer chacun des articles, il faut adresser les balises\n",
    "`<article>` contenues dans le sélecteur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [4]: response.css(\".river\")[0].css(\".teaser\")\n",
    "Out[4]:\n",
    "[<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi mg'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>,\n",
    "<Selector xpath='descendant-or-self::article' data='<article class=\"grid_12 alpha enrichi\">\\n'>]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme précédemment, on peut empiler les sélecteurs `css` pour créer des\n",
    "requêtes plus complexes.\n",
    "\n",
    "Par exemple, pour récupérer tous les titres des différents articles :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [8]: response.css(\".river\")[0].css(\".teaser h3::text\").extract()\n",
    "Out[8]:\n",
    "['Des dizaines de milliers de Géorgiens contestent dans la rue l’élection de Salomé Zourabichvili\\r\\n\\r\\n\\r\\n',\n",
    "'A Budapest en Hongrie, un îlot décroissant pour favoriser la transition\\r\\n\\r\\n\\r\\n',\n",
    "'En Israël, la police recommande l’inculpation de Nétanyahou dans une troisième enquête\\r\\n\\r\\n\\r\\n',\n",
    "'Donald Trump veut «\\xa0mettre fin\\xa0» à l’Aléna rapidement\\r\\n\\r\\n\\r\\n',\n",
    "'Le cauchemar de la «\\xa0rééducation\\xa0» des musulmans en Chine\\r\\n\\r\\n',\n",
    "'\\r\\n',\n",
    "'«\\xa0AMLO\\xa0» lance sa transformation du Mexique\\r\\n\\r\\n\\r\\n',\n",
    "'«\\xa0Paris brûle\\xa0»\\xa0: les médias étrangers relatent le «\\xa0chaos\\xa0» en marge des défilés des «\\xa0gilets jaunes\\xa0»\\r\\n\\r\\n\\r\\n',\n",
    "'Andrés Manuel Lopez Obrador intronisé président du Mexique\\r\\n\\r\\n\\r\\n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En HTML les données sont souvent de très mauvaise qualité. Il faut\n",
    "définir des méthodes permettant de les nettoyer pour être intégrées dans\n",
    "des bases de données.\n",
    "\n",
    "Par exemple, pour supprimer tous les espaces superflus :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_spaces(string_):\n",
    "    if string_ is not None: \n",
    "        return \" \".join(string_.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'appliquer à tous les titres récupérés, on peut faire une list\n",
    "comprehension : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [11]: [clean_spaces(article) for article in response.css(\".river\")[0].css(\".teaser h3::text\").extract()]  \n",
    "\n",
    "Out[11]: ['Des dizaines de milliers de Géorgiens contestent dans la rue l’élection de Salomé Zourabichvili',\n",
    "          'A Budapest en Hongrie, un îlot décroissant pour favoriser la transition', \n",
    "          'En Israël, la police recommande l’inculpation de Nétanyahou dans une troisième enquête',\n",
    "          'Donald Trump veut « mettre fin » à l’Aléna rapidement', \n",
    "          'Le cauchemar de la « rééducation » des musulmans en Chine',\n",
    "          '',\n",
    "          '« AMLO » lance sa transformation du Mexique', \n",
    "          '« Paris brûle » : les médias étrangers relatent le « chaos » en marge des défilés des « gilets jaunes »',\n",
    "          'Andrés Manuel Lopez Obrador intronisé président du Mexique'\n",
    "         ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode précédente est intéressante si l'on ne recherche qu'une seule\n",
    "information par article.\n",
    "\n",
    "Par contre si l'on veut récupérer d'autres caractéristiques comme\n",
    "l'image ou la description par exemple, il est plus intéressant et plus\n",
    "efficace de récupérer l'objet et d'effectuer plusieurs traitements sur\n",
    "ce dernier.\n",
    "\n",
    "Chaque objet retourné par les requêtes `css` est un selecteur avec\n",
    "lequel on peut interagir.\n",
    "\n",
    "Par exemple pour récupérer le titre et le prix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In [25]: for article in response.css(\".fleuve\")[0].css(\"article\"):\n",
    "...:     title = clean_spaces(article.css(\"h3 a::text\").extract_first())\n",
    "...:     image = article.css(\"img::attr(data-src)\").extract_first()\n",
    "...:     description = article.css(\".txt3::text\").extract_first()\n",
    "...:     print(f\"Title {title} \\nImage {image}\\nDescription {description}\\n ----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title Des dizaines de milliers de Géorgiens contestent dans la rue l’élection de Salomé Zourabichvili\n",
    "Image https://s1.lemde.fr/image/2018/12/02/147x97/5391641_7_5874_les-partisans-de-l-opposant-grigol-vashadze_20d2e8693a49b83fd3c5578f7799ae9c.jpg\n",
    "Description Elue présidente (un rôle essentiellement symbolique en Géorgie), l’ex-diplomate française, candidate du pouvoir, est contestée par l’opposition.\n",
    "----\n",
    "Title A Budapest en Hongrie, un îlot décroissant pour favoriser la transition\n",
    "Image https://img.lemde.fr/2018/12/01/10/0/4214/2809/147/97/60/0/15b32ca_1EY4qISQ_BP4kPAh1fozJdXZ.jpg\n",
    "Description Le centre logistique Cargonomia sert de matrice aux coopératives de l’économie durable et solidaire hongroise.\n",
    "----\n",
    "Title En Israël, la police recommande l’inculpation de Nétanyahou dans une troisième enquête\n",
    "Image https://img.lemde.fr/2018/12/02/167/0/4207/2804/147/97/60/0/9e02c9b_3580d043ebc94b48b0f2cfef4e9a21e7-3580d043ebc94b48b0f2cfef4e9a21e7-0.jpg\n",
    "Description Le premier ministre est soupçonné de corruption, fraude et abus de pouvoir, dans une affaire impliquant le groupe de télécoms israélien Bezeq.\n",
    "----\n",
    "Title Donald Trump veut « mettre fin » à l’Aléna rapidement\n",
    "Image https://img.lemde.fr/2018/11/30/0/0/4861/3240/147/97/60/0/8b87184_5826023-01-06.jpg\n",
    "Description Le président américain souhaite voir disparaître l’accord de libre-échange remontant à 1994 avec le Mexique et le Canada, qu’il qualifie régulièrement de « pire accord jamais signé », en faveur du nouveau traité négocié difficilement avec ses voisins nord-américains ces derniers mois.\n",
    "----\n",
    "Title Le cauchemar de la « rééducation » des musulmans en Chine\n",
    "Image https://img.lemde.fr/2018/11/15/151/0/5000/3333/147/97/60/0/118c78f_248b226e6b91450aa8a68bd0ea5525a8-248b226e6b91450aa8a68bd0ea5525a8-0.jpg\n",
    "Description Ouïgours et Kazakhs du Xinjiang... C’est toute une population musulmane que Pékin veut « rééduquer » en internant des centaines de milliers d’entre eux dans des camps.\n",
    "----\n",
    "Title « AMLO » lance sa transformation du Mexique\n",
    "Image https://img.lemde.fr/2018/12/02/45/0/1497/998/147/97/60/0/a33c174_GGGTBR84_MEXICO-POLITICS-_1202_11.JPG\n",
    "Description Education et santé gratuites, hausse du salaire minimum, bourses scolaires : à peine investi, le président Andres Manuel Lopez Obrador a listé les mesures qu’il entend prendre pour redresser le pays.\n",
    "----\n",
    "Title « Paris brûle » : les médias étrangers relatent le « chaos » en marge des défilés des « gilets jaunes »\n",
    "Image https://img.lemde.fr/2018/12/02/361/0/598/396/147/97/60/0/ba46a6e_XVIt1Ffwm50iYBheccVieUQQ.jpg\n",
    "Description Les images de destructions, d’échauffourées ou de voitures enflammées s’affichaient samedi soir en « une » de nombreux sites d’actualité internationaux.\n",
    "----\n",
    "Title Andrés Manuel Lopez Obrador intronisé président du Mexique\n",
    "Image https://img.lemde.fr/2018/12/02/91/145/1346/897/147/97/60/0/877cd51_a4618baa8da2414bb62bab28a6d4c745-a4618baa8da2414bb62bab28a6d4c745-0.jpg\n",
    "Description Le nouveau chef d’Etat a promis de lutter contre la corruption en menant une transformation « profonde et radicale » du pays.\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence des données\n",
    "\n",
    "Pour pouvoir stocker les informations que l'on récupère en parcourant un\n",
    "site il faut pouvoir les stocker. On utilise soit de simples\n",
    "dictionnaires Python, ou mieux des `scrapy.Item` qui sont des\n",
    "dictionnaires améliorés.\n",
    "\n",
    "Nous allons voir les deux façons de faire. On peut réécrire la méthode\n",
    "`parse_category()` pour lui faire retourner un dictionnaire\n",
    "correspondant à chaque offre rencontrée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_category(self, response):\n",
    "    for article in response.css(\".fleuve\")[0].css(\"article\"):\n",
    "        title = self.clean_spaces(article.css(\"h3 a::text\").extract_first())\n",
    "        image = article.css(\"img::attr(data-src)\").extract_first()\n",
    "        description = article.css(\".txt3::text\").extract_first()\n",
    "        yield {\n",
    "            \"title\":title,\n",
    "            \"image\":image,\n",
    "            \"description\":description\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si on combine tout dans la spider :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load newscrawler/newscrawler/spiders/lemonde_v3.py\n",
    "import scrapy\n",
    "from scrapy import Request\n",
    "\n",
    "\n",
    "class LemondeSpider(scrapy.Spider):\n",
    "    name = \"lemondev3\"\n",
    "    allowed_domains = [\"www.lemonde.fr\"]\n",
    "    start_urls = ['https://www.lemonde.fr']\n",
    "\n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').extract_first()\n",
    "        all_links = {\n",
    "            name:response.urljoin(url) for name, url in zip(\n",
    "            response.css(\"#nav-markup .Nav__item\")[3].css(\"a::text\").extract(),\n",
    "            response.css(\"#nav-markup .Nav__item\")[3].css(\"a::attr(href)\").extract())\n",
    "        }\n",
    "        for link in all_links.values():\n",
    "            yield Request(link, callback=self.parse_category)\n",
    "            \n",
    "    def parse_category(self, response):\n",
    "        for article in response.css(\".river\")[0].css(\".teaser\"):\n",
    "            title = self.clean_spaces(article.css(\"h3::text\").extract_first())\n",
    "            image = article.css(\"img::attr(data-src)\").extract_first()\n",
    "            description = article.css(\".txt3::text\").extract_first()\n",
    "            yield {\n",
    "                \"title\":title,\n",
    "                \"image\":image,\n",
    "                \"description\":description\n",
    "            }\n",
    "\n",
    "    def clean_spaces(self, string):\n",
    "        if string:\n",
    "            return \" \".join(string.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant lancer notre spider avec la commande suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl <NAME>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scrapy crawl` permet de démarrer le processus en allant chercher la\n",
    "classe `scrapy.Spider` dont l'attribut `name` = &lt;NAME&gt;.\n",
    "\n",
    "Par exemple, pour la spider `LeMondeSpider` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd newscrawler && scrapy crawl lemondev3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut exporter les résultats de ces retours dans différents formats de\n",
    "fichiers.\n",
    "\n",
    "-   CSV : `scrapy crawl lemonde -o lbc.csv`\n",
    "-   JSON : `scrapy crawl lemonde -o lbc.json`\n",
    "-   JSONLINE : `scrapy crawl lemonde -o lbc.jl`\n",
    "-   XML : `scrapy crawl lemonde -o lbc.xml`\n",
    "\n",
    "### Exercice :\n",
    "\n",
    "Exécuter la spider avec les différents formats de stockage.\n",
    "Explorer ensuite le contenu des fichiers ainsi créés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Votre premier Item\n",
    "\n",
    "La classe `Item` permet de structurer les données que l'on souhaite\n",
    "récupérer sous la forme d'un modèle. Les items doivent être définis dans\n",
    "le fichier `items.py` créé par la commande `scrapy startproject`. Les\n",
    "`Item` héritent de la class `scrapy.Item`.\n",
    "\n",
    "On veut structurer les données avec deux champs : le titre et le prix de\n",
    "l'annonce. Scrapy utilise une classe `scrapy.Field` permettant de\n",
    "'déclarer' ces champs. Dans notre cas :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load newscrawler/newscrawler/items.py\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class ArticleItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    image = scrapy.Field()\n",
    "    description = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliser la classe `scrapy.Item` plutôt qu'un simple dictionnaire permet\n",
    "plus de contrôle sur la structure des données. En effet, on ne peut\n",
    "insérer dans les items que des données avec des clés 'déclarées'. Ce qui\n",
    "assure une plus grande cohérence au sein d'un projet.\n",
    "\n",
    "On peut instancier un item de plusieurs façons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_item = ArticleItem(title=\"Gilets Jaunes\", image=None, description=\"Un samedi de manifestations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_item = ArticleItem()\n",
    "article_item[\"title\"] = 'Gilets Jaunes'\n",
    "article_item[\"description\"] = 'Un samedi de manifestations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(article_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La définition d'un item permet de palier toutes les erreurs de typo dans\n",
    "les champs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_item = ArticleItem()\n",
    "article_item[\"titelkjwnxvmnscbvmknxc\"] = 'Gilets Jaunes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les items héritent des dictionnaires Python, et possèdent donc toutes\n",
    "les méthodes de ceux-ci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_item = ArticleItem(title=\"Gilets Jaunes\")\n",
    "print(article_item[\"title\"]) # Méthode __getitem__()\n",
    "print(article_item.get(\"description\", \"no description provided\")) # Méthode get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut transformer un `Item` en dictionnaire très facilement, en le\n",
    "passant au constructeur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_item = ArticleItem(title=\"Drone DJI\")\n",
    "print(type(article_item))\n",
    "dict_item = dict(article_item)\n",
    "print(type(dict_item))\n",
    "print(dict_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On intègre maintenant cet item dans notre spider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "from scrapy import Request\n",
    "from ..items import ArticleItem\n",
    "class LemondeSpider(scrapy.Spider):\n",
    "    name = \"lemonde\"\n",
    "    allowed_domains = [\"www.lemonde.fr\"]\n",
    "    start_urls = ['https://www.lemonde.fr']\n",
    "\n",
    "    def parse(self, response):\n",
    "        title = response.css('title::text').extract_first()\n",
    "        all_links = {\n",
    "            name:response.urljoin(url) for name, url in zip(\n",
    "            response.css(\"#nav-markup .Nav__item\")[3].css(\"a::text\").extract(),\n",
    "            response.css(\"#nav-markup .Nav__item\")[3].css(\"a::attr(href)\").extract())\n",
    "        }\n",
    "        for link in all_links.values():\n",
    "            yield Request(link, callback=self.parse_category)\n",
    "\n",
    "    def parse_category(self, response):\n",
    "        for article in response.css(\".fleuve\")[0].css(\"article\"):\n",
    "            title = self.clean_spaces(article.css(\"h3 a::text\").extract_first())\n",
    "            image = article.css(\"img::attr(data-src)\").extract_first()\n",
    "            description = article.css(\".txt3::text\").extract_first()\n",
    "\n",
    "            yield ArticleItem(\n",
    "                title=title,\n",
    "                image=image,\n",
    "                description=description\n",
    "            )\n",
    "\n",
    "    def clean_spaces(self, string):\n",
    "        if string:\n",
    "            return \" \".join(string.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit bien que le générateur retourne maintenant un `Item`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice : \n",
    "\n",
    "Relancer la spider pour vérifier le bon déroulement de l'extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd newscrawler/ && scrapy crawl lemondev4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing\n",
    "\n",
    "Si l'on se réfère au diagramme d'architecture de Scrapy, on voit qu'il\n",
    "est possible d'insérer des composants supplémentaires dans le flux de\n",
    "traitement. Ces composants s'appellent `Pipelines`.\n",
    "\n",
    "Par défaut, tous les `Item` générés au sein d'un projet Scrapy passent\n",
    "par les `Pipelines`. Les pipelines sont utilisés la plupart du temps\n",
    "pour :\n",
    "\n",
    "-   Nettoyer du contenu HTML ;\n",
    "-   Valider les données scrapées ;\n",
    "-   Supprimer les items qu'on ne souhaite pas stocker ;\n",
    "-   Stocker ces objets dans des bases de données.\n",
    "\n",
    "Les pipelines doivent être définis dans le fichier `pipelines.py`.\n",
    "\n",
    "Dans notre cas on peut vouloir nettoyer le champ `title` pour enlever\n",
    "les caractères superflus.\n",
    "\n",
    "Nous allons alors transferer la fonction de nettoyage du code html dans\n",
    "une Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load newscrawler/newscrawler/pipelines.py\n",
    "\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "from scrapy.exceptions import DropItem\n",
    "\n",
    "class TextPipeline(object):\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        if item['title']:\n",
    "            item[\"title\"] = clean_spaces(item[\"title\"])\n",
    "            return item\n",
    "        else:\n",
    "            raise DropItem(\"Missing title in %s\" % item)\n",
    "\n",
    "\n",
    "def clean_spaces(string):\n",
    "    if string:\n",
    "        return \" \".join(string.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour dire au process Scrapy de faire transiter les items par ces\n",
    "pipelines. Il faut le spécifier dans le fichier de paramétrage\n",
    "`settings.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_PIPELINES = {\n",
    "     'newscrawler.pipelines.TextPipeline': 300,\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut maintenant supprimer la fonction `clean_spaces()` de\n",
    "l'extraction des données et laisser le Pipeline faire son travail. La\n",
    "valeur entière définie permet de déterminer l'ordre dans lequel les\n",
    "pipelines vont être appelés. Ces entiers peuvent être compris entre 0 et\n",
    "1000.\n",
    "\n",
    "On relance notre spider :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl lemonde -o ../data/articles.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut aussi utiliser les Pipelines pour stocker les données récupérées\n",
    "dans une base de données. Pour stocker les items dans des documents\n",
    "mongo :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "class MongoPipeline(object):\n",
    "\n",
    "    collection_name = 'scrapy_items'\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.client = pymongo.MongoClient()\n",
    "        self.db = self.client[\"lemonde\"]\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.client.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.db[self.collection_name].insert_one(dict(item))\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on redéfinit deux autres méthodes: `open_spider()`et\n",
    "`close_spider()`, ces méthodes sont appelées comme leur nom l'indique,\n",
    "lorsque la Spider est instanciée et fermée.\n",
    "\n",
    "Ces méthodes nous permettent d'ouvrir la connexion Mongo et de la fermer\n",
    "lorsque le scraping se termine. La méthode `process_item()` est appelé à\n",
    "chaque fois qu'un item passe dans le mécanisme interne de scrapy. Ici,\n",
    "la méthode permet d'insérer l'item en tant que document mongo.\n",
    "\n",
    "Pour que ce pipeline soit appelé il faut l'ajouter dans les settings du\n",
    "projet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_PIPELINES = {\n",
    "    'newscrawler.pipelines.TextPipeline': 100,\n",
    "    'newscrawler.pipelines.MongoPipeline': 300\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le pipeline est ajoutée à la fin du process pour profiter des deux\n",
    "précédents.\n",
    "\n",
    "## Settings\n",
    "\n",
    "Scrapy permet de gérer le comportement des spiders avec certains\n",
    "paramètres. Comme expliqué dans le premier cours, il est important de\n",
    "suivre des règles en respectant la structure des différents sites. Il\n",
    "existe énormément de paramètres mais nous allons (dans le cadre de ce\n",
    "cours) aborder les plus utilisés :\n",
    "\n",
    "-   DOWNLOAD\\_DELAY : Le temps de téléchargement entre chaque requête\n",
    "    sur le même domaine ;\n",
    "-   CONCURRENT\\_REQUESTS\\_PER\\_DOMAIN : Nombre de requêtes simultanées\n",
    "    par domaine ;\n",
    "-   CONCURRENT\\_REQUESTS\\_PER\\_IP : Nombre de requêtes simultanées par\n",
    "    IP ;\n",
    "-   DEFAULT\\_REQUEST\\_HEADERS : Headers HTTP utilisés pour les requêtes\n",
    "    ;\n",
    "-   ROBOTSTXT\\_OBEY : Scrapy récupère le robots.txt et adapte le\n",
    "    scraping en fonction des règles trouvées ;\n",
    "-   USER\\_AGENT : UserAgent utilisé pour faire les requêtes ;\n",
    "-   BOT\\_NAME : Nom du bot annoncé lors des requêtes\n",
    "-   HTTPCACHE\\_ENABLED : Utilisation du cache HTTP, utile lors du\n",
    "    parcours multiple de la même page.\n",
    "\n",
    "Le fichiers `settings.py` permet de définir les paramètres globaux d'un\n",
    "projet. Si votre projet contient un grand nombre de spiders, il peut\n",
    "être intéressant d'avoir des paramètres distincts pour chaque spider. Un\n",
    "moyen simple est d'ajouter un attribut `custom_settings` à votre spider\n",
    ":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeMondeSpider(scrapy.Spider):\n",
    "        name = \"lemonde\"\n",
    "        allowed_domains = [\"lemonde.fr\"]\n",
    "        start_urls = ['http://lemonde.fr/']\n",
    "        custom_settings = {\n",
    "            \"HTTPCACHE_ENABLED\":True, \n",
    "            \"CONCURRENT_REQUESTS_PER_DOMAIN\":100\n",
    "        }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
